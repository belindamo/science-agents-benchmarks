{"id":"paper_1702500000000","title":"Attention Is All You Need","authors":"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin","journal":"NeurIPS","year":"2017","doi":"10.5555/3295222.3295349","url":"https://arxiv.org/abs/1706.03762","keyAssumptions":"RNNs and CNNs are necessary for sequence transduction; Sequential computation limits parallelization","citation":"Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.","notes":"Foundational paper introducing the Transformer architecture. Key innovation: self-attention mechanism allows modeling dependencies without recurrence or convolution.","addedDate":"2024-01-15T10:00:00Z"}
{"id":"paper_1702500000001","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","authors":"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova","journal":"NAACL","year":"2019","doi":"10.18653/v1/N19-1423","url":"https://arxiv.org/abs/1810.04805","keyAssumptions":"Unidirectional language models are sufficient for pre-training; Fine-tuning requires task-specific architectures","citation":"Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT (pp. 4171-4186).","notes":"Introduces bidirectional pre-training using masked language modeling. Shows that deep bidirectional representations significantly improve downstream task performance.","addedDate":"2024-01-15T10:05:00Z"}
{"id":"paper_2025080500001","title":"An Automated Benchmark for Scientific Discovery in LLMs","authors":"Tingting Chen, Srinivas Anumasa, Beibei Lin, Vedant Shah, Anirudh Goyal, Dianbo Liu","journal":"arXiv","year":"2025","doi":"arXiv:2502.15224","url":"https://arxiv.org/abs/2502.15224","keyAssumptions":"General language capabilities translate to scientific discovery; Static evaluation captures discovery processes","citation":"Chen, T., Anumasa, S., Lin, B., Shah, V., Goyal, A., & Liu, D. (2025). An Automated Benchmark for Scientific Discovery in LLMs. arXiv preprint arXiv:2502.15224.","notes":"First standardized benchmark for scientific discovery in LLMs based on causal graph discovery principles. Interactive oracle-based framework shows significant performance drops with complexity.","addedDate":"2025-08-05T20:58:00Z"}
{"id":"paper_2025080500002","title":"When AI Co-Scientists Fail: SPOT-a Benchmark for Academic Verification","authors":"Guijin Son, Jiwoo Hong, Honglu Fan, Heejeong Nam, Hyunwoo Ko, Seungwon Lim, Jinyeop Song, Jinha Choi, Gon√ßalo Paulo, Youngjae Yu, Stella Biderman","journal":"arXiv","year":"2025","doi":"arXiv:2505.11855","url":"https://arxiv.org/abs/2505.11855","keyAssumptions":"Generative capabilities translate to verification abilities; LLMs can reliably detect scientific errors","citation":"Son, G., Hong, J., Fan, H., Nam, H., Ko, H., Lim, S., ... & Biderman, S. (2025). When AI Co-Scientists Fail: SPOT-a Benchmark for Academic Verification. arXiv preprint arXiv:2505.11855.","notes":"Benchmark for AI scientific manuscript verification using real published errors. Best model achieves only 21.1% recall, highlighting gaps in verification capability.","addedDate":"2025-08-05T20:58:00Z"}
{"id":"paper_2025080500003","title":"PaperBench: Evaluating AI's Ability to Replicate AI Research","authors":"Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Chan Jun Shern, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, Tejal Patwardhan","journal":"arXiv","year":"2025","doi":"arXiv:2504.01848","url":"https://arxiv.org/abs/2504.01848","keyAssumptions":"Code generation abilities enable research replication; Component evaluation predicts system performance","citation":"Starace, G., Jaffe, O., Sherburn, D., Aung, J., Shern, C. J., Maksin, L., ... & Patwardhan, T. (2025). PaperBench: Evaluating AI's Ability to Replicate AI Research. arXiv preprint arXiv:2504.01848.","notes":"Comprehensive benchmark for autonomous research replication with hierarchical task decomposition. Best agent achieves 21.0% replication score vs human experts.","addedDate":"2025-08-05T20:58:00Z"}
{"id":"paper_2025080500004","title":"DiscoveryBench: Towards Data-Driven Discovery with Large Language Models","authors":"Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, Peter Clark","journal":"arXiv","year":"2024","doi":"arXiv:2407.01725","url":"https://arxiv.org/abs/2407.01725","keyAssumptions":"Coding and analysis capabilities enable hypothesis discovery; Static datasets sufficient for discovery evaluation","citation":"Majumder, B. P., Surana, H., Agarwal, D., Mishra, B. D., Meena, A., Prakhar, A., ... & Clark, P. (2024). DiscoveryBench: Towards Data-Driven Discovery with Large Language Models. arXiv preprint arXiv:2407.01725.","notes":"First comprehensive benchmark for data-driven discovery with tasks from real research workflows. Best system achieves only 25% success rate on discovery tasks.","addedDate":"2025-08-05T20:58:00Z"}
{"id":"paper_2025080500005","title":"Holistic Evaluation of Language Models","authors":"Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, et al.","journal":"TMLR","year":"2023","doi":"arXiv:2211.09110","url":"https://arxiv.org/abs/2211.09110","keyAssumptions":"Narrow accuracy evaluation sufficient for model assessment; Individual benchmarks provide adequate model comparison","citation":"Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., ... & Manning, C. D. (2023). Holistic Evaluation of Language Models. Transactions on Machine Learning Research.","notes":"Established holistic evaluation paradigm across accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency dimensions. Foundational framework for comprehensive model assessment.","addedDate":"2025-08-05T20:58:00Z"}
{"id":"paper_2025080500006","title":"How Far Are AI Scientists from Changing the World?","authors":"Qiujie Xie, Yixuan Weng, Minjun Zhu, Fuchen Shen, Shulin Huang, Zhen Lin, Jiahui Zhou, Zilan Mao, Zijie Yang, Linyi Yang, Jian Wu, Yue Zhang","journal":"arXiv","year":"2025","doi":"arXiv:2507.23276","url":"https://arxiv.org/html/2507.23276v1","keyAssumptions":"Individual AI Scientist systems can be evaluated in isolation; Component capabilities predict breakthrough potential","citation":"Xie, Q., Weng, Y., Zhu, M., Shen, F., Huang, S., Lin, Z., ... & Zhang, Y. (2025). How Far Are AI Scientists from Changing the World? arXiv preprint arXiv:2507.23276.","notes":"Comprehensive survey of AI Scientist systems with three-level autonomy framework. Identifies critical gaps between current capabilities and world-changing scientific AI.","addedDate":"2025-08-05T20:58:00Z"}