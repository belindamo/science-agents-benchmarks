{"id":"run_1702600000000","experimentId":"exp_transformer_ablation","title":"Self-Attention Head Ablation Study","status":"completed","startDate":"2024-01-20T14:00:00Z","endDate":"2024-01-22T18:30:00Z","codeUrl":"experiments/transformer_ablation_001/run/train.py","dataPath":"data/wmt14_en_de_preprocessed/","hyperparameters":{"model":"transformer_base","num_heads":[1,2,4,8,16],"d_model":512,"num_layers":6,"learning_rate":0.0001,"batch_size":32,"max_epochs":50},"metrics":{"final_bleu":[18.2,22.5,26.8,27.1,27.0],"training_time_hours":[12.5,13.2,14.8,16.3,18.1]},"notes":"Ablation study showing diminishing returns beyond 8 attention heads. 4-8 heads appears to be the sweet spot for this model size.","createdDate":"2024-01-20T13:00:00Z"}
{"id":"run_1702600000001","experimentId":"exp_1702400000000","title":"DistilBERT vs TinyBERT Baseline","status":"running","startDate":"2024-01-25T09:00:00Z","endDate":"","codeUrl":"experiments/efficient_transformer_001/run/baseline.py","dataPath":"data/glue_benchmark/","hyperparameters":{"models":["bert-base","distilbert","tinybert"],"batch_size":32,"eval_batch_size":64,"device":"A100"},"metrics":{"progress":"60%","glue_scores":{"bert-base":{"cola":52.1,"sst2":92.5,"mrpc":88.9},"distilbert":{"cola":48.2,"sst2":91.3,"mrpc":86.5}}},"notes":"Running baseline comparison. TinyBERT evaluation pending. GPU memory usage significantly lower for distilled models.","createdDate":"2024-01-25T08:30:00Z"}
{"id":"run_2025080500001","experimentId":"exp_2025080500001","title":"LLM-Human Judge Correlation Analysis for Scientific Evaluation","status":"completed","startDate":"2025-08-05T21:52:00Z","endDate":"2025-08-05T21:55:00Z","codeUrl":"experiments/llm_human_judge_001/run/simple_experiment.py","dataPath":"data/llm_judge_results.json","hyperparameters":{"n_papers":50,"llm_models":["gpt-4","claude-3.5-sonnet","gemini-pro"],"evaluation_dimensions":["methodology","reproducibility","novelty","significance","impact"],"correlation_threshold_structured":0.7,"correlation_threshold_subjective":0.5},"metrics":{"structured_correlation_avg":0.747,"subjective_correlation_avg":0.382,"h1_validated":true,"h2_validated":true,"best_model":"claude-3.5-sonnet"},"notes":"Successfully validated bit flip hypothesis: LLMs excel at structured evaluation (r=0.747) but struggle with subjective assessment (r=0.382). Supports human-AI collaboration model for scientific peer review.","createdDate":"2025-08-05T21:52:00Z"}