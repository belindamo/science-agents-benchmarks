# Experiment Ideas


i was thinking paperbench's approach works quite well with LLM judges (https://openai.com/index/paperbench/) and we can take inspo from it. 

they benchmarked with LLM judge
then i was thinking what we could do is get human ratings too, from like 3 experts. and then compare correlation between humans and LLMs+humans in their judge rankings




---
*This section is being enhanced by The Research Company AI Agent*
